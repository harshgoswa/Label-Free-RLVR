# Label-Free Reinforcement Learning: A Comprehensive Resource ðŸ“š

Welcome to the **Label-Free-RLVR** repository! This is your go-to source for the latest and most impactful research papers on label-free reinforcement learning. Our aim is to provide a curated collection of works that highlight the advancements in this exciting area of study.

## Authors

This repository is maintained by:

- [Qingyang Zhang](https://qingyangzhang.github.io)
- [Haitao Wu](https://haitaowutju.github.io)
- [Yi Ding](https://dripnowhy.github.io)

If you notice any papers we may have overlooked, please feel free to reach out!

## Important Note

This repository serves researchers interested in the rapid developments in the reinforcement learning (RL) field. Many recent papers claim to enhance the reasoning abilities of language models. However, as highlighted in [this recent work](https://safe-lip-9a8.notion.site/Incorrect-Baseline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f1fbf0ee8094ab8ded1953c15a37#2022f1fbf0ee80cb9b18f7eac460410a), the improvements reported in many Reinforcement Learning with Verifiable Reward (RLVR) papers may not be as robust as they appear. Various issues in evaluation setups could lead to misleading conclusions. The baseline performance of pre-RL models may be significantly underreported. Thus, it is crucial to scrutinize the extent of genuine learning that occurs.

## Table of Contents

1. [Getting Started](#getting-started)
2. [Research Papers](#research-papers)
3. [Contributing](#contributing)
4. [License](#license)
5. [Releases](#releases)
6. [Contact](#contact)

## Getting Started

To explore the repository, you can clone it using the following command:

```bash
git clone https://github.com/harshgoswa/Label-Free-RLVR.git
```

Once cloned, navigate to the repository folder:

```bash
cd Label-Free-RLVR
```

Here, you will find a collection of research papers and relevant resources.

## Research Papers

### Before DeepSeek-R1-Zero

- **[Preference Optimization for Reasoning with Pseudo Feedback](https://link_to_paper.com)**

This paper discusses the optimization of preferences to enhance reasoning capabilities using pseudo feedback mechanisms. The findings suggest innovative methods for improving learning outcomes in RL scenarios.

### Additional Papers

We will continue to update this section with more papers as they become available. Each entry will include a brief description and a link to the full text.

## Contributing

We welcome contributions from the community. If you would like to add a paper or provide feedback, please follow these steps:

1. Fork the repository.
2. Create a new branch (`git checkout -b feature/YourFeature`).
3. Make your changes and commit them (`git commit -m 'Add some feature'`).
4. Push to the branch (`git push origin feature/YourFeature`).
5. Open a pull request.

Your contributions will help improve the resource for everyone in the research community.

## License

This repository is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.

## Releases

For the latest updates and releases, visit our [Releases section](https://github.com/harshgoswa/Label-Free-RLVR/releases). You can download and execute the files provided there to access new features and improvements.

## Contact

If you have any questions or suggestions, please reach out to the authors via their personal websites linked above. We appreciate your feedback and collaboration!

---

Thank you for visiting the **Label-Free-RLVR** repository! We hope you find this collection of papers helpful in your research endeavors.